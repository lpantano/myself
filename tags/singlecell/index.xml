<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>singlecell on My BioBits</title>
    <link>http://lpantano.github.io/tags/singlecell/</link>
    <description>Recent content in singlecell on My BioBits</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 11 Jul 2019 00:00:00 +0000</lastBuildDate>
    
	    <atom:link href="http://lpantano.github.io/tags/singlecell/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>How to set up cellranger to make your hpc admin happy</title>
      <link>http://lpantano.github.io/post/2019/2019-07-12-cellranger-efficiency-in-hpc-copy/</link>
      <pubDate>Thu, 11 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>http://lpantano.github.io/post/2019/2019-07-12-cellranger-efficiency-in-hpc-copy/</guid>
      <description>

&lt;p&gt;I found myself to force to use &lt;a href=&#34;https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/advanced/cluster-mode&#34; target=&#34;_blank&#34;&gt;cellranger&lt;/a&gt;. Meanwhile it helps a lot to
run from &lt;a href=&#34;https://www.illumina.com/informatics/sequencing-data-analysis/sequence-file-formats.html&#34; target=&#34;_blank&#34;&gt;bcl&lt;/a&gt; files to single cell counts matrixes, I discovered that is
quite difficult to control many options related to optimization.&lt;/p&gt;

&lt;p&gt;I have to run more than 200 samples in a short time of period. In my current
position at MIT, I joined the OpenMind cluster in the McGovern institute. I was
pleased to find a very flexible cluster, but as any other cluster you need to
respect other users.&lt;/p&gt;

&lt;p&gt;My samples are human samples, so I need a lot of memory so &lt;a href=&#34;https://github.com/alexdobin/STAR&#34; target=&#34;_blank&#34;&gt;STAR&lt;/a&gt; can run. The
command line is easy to set up:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;cellranger count --id=SAMPLE --transcriptome=GRCh38_pre_mRNA --fastqs=PATH2FASTQS --sample=SAMPLE&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Of course, that is in local mode. If you want to set up a job to a cluster,
it would look like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/bash
#SBATCH -N 1
#SBATCH -c 6
#SBATCH --mem=12000
#SBATCH -t 0-48:00:00
#SBATCH -J cellr
#SBATCH -e job-counts.e
#SBATCH -o job-counts.o
## SBATCH --mail-type=END,FAIL # this line is commented
## SBATCH --mail-user=user@mit.edu  # this line is commented

cellranger count --id=SAMPLE --transcriptome=GRCh38_pre_mRNA \
          --fastqs=PATH2FASTQS --sample=SAMPLE \
          --localcores=6 --localmem=120

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The main problem with that is you are using 120G during all the process that could
go to 1-2 days.&lt;/p&gt;

&lt;p&gt;I discovered that you can setup the &lt;code&gt;cluster&lt;/code&gt; mode to make &lt;code&gt;cellranger&lt;/code&gt; to send
jobs to the cluster. However, &lt;code&gt;slurm&lt;/code&gt; is not a option to the documentation, BUT
you can specify a template file:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/bash
#SBATCH -J __MRO_JOB_NAME__
#SBATCH -N 1
#SBATCH -c __MRO_THREADS__
#SBATCH --mem=__MRO_MEM_GB__G
#SBATCH -o __MRO_STDOUT__
#SBATCH -e __MRO_STDERR__
#SBATCH -t 0-48:00:00
#SBATCH --qos=&amp;quot;normal&amp;quot;

 __MRO_CMD__
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The command would look like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cellranger count --id=SAMPLE --transcriptome=GRCh38_pre_mRNA \
          --fastqs=PATH2FASTQS --sample=SAMPLE \
          --jobmode=slurm.template \
          --maxjobs=3 --jobinterval=1000

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I was feeling victory, ha ha ha, BUT I saw that the &lt;code&gt;ALIGN&lt;/code&gt; step was requesting
220G of memory&amp;hellip;. WHAT!!!!!????? or better &lt;a href=&#34;https://www.destroyallsoftware.com/talks/wat&#34; target=&#34;_blank&#34;&gt;WATTT????!!!!!&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;note&lt;/strong&gt;: this is set up during genome generation with the parameters:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
    --memgb=&amp;lt;num&amp;gt;       Maximum memory (GB) used when aligning reads with STAR.
                          Defaults to 16.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;note&lt;/strong&gt;: default is 16G but if I use that then my jobs are cancelled because using more memory than requested, as I expected considering STAR needs more than that for human genome.&lt;/p&gt;

&lt;p&gt;I guess I didn&amp;rsquo;t look at the first place I should have looked, but it took me
some time to discovered that the memory is coming from a file in the reference
genome folder:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat GRCh38_pre_mRNA/reference.json
{
    &amp;quot;fasta_hash&amp;quot;: &amp;quot;954a9c3916ef1544c9358445440b9683fc061c71&amp;quot;,
    &amp;quot;genomes&amp;quot;: [
        &amp;quot;GRCh38_pre_mRNA&amp;quot;
    ],
    &amp;quot;gtf_hash&amp;quot;: &amp;quot;12aae438159b35282adaf643cca4dc5887b0448a&amp;quot;,
    &amp;quot;input_fasta_files&amp;quot;: [
        &amp;quot;Homo_sapiens.GRCh38.dna.toplevel.fa&amp;quot;
    ],
    &amp;quot;input_gtf_files&amp;quot;: [
        &amp;quot;GRC38_preRNA.gtf&amp;quot;
    ],
    &amp;quot;mem_gb&amp;quot;: 220,
    &amp;quot;mkref_version&amp;quot;: &amp;quot;3.0.2&amp;quot;,
    &amp;quot;threads&amp;quot;: 24,
    &amp;quot;version&amp;quot;: null
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I realized that when I found, after 100 &lt;code&gt;ls/cat/less/grep&lt;/code&gt;, this file with these
lines of code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cellranger-3.0.2/cellranger-cs/3.0.2/lib/python/cellranger/utils.py
def _load_reference_metadata_file(reference_path):
    reference_metadata_file = os.path.join(reference_path, cr_constants.REFERENCE_METADATA_FILE)
    with open(reference_metadata_file, &#39;r&#39;) as f:
        return json.load(f)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So, easy peasy, I changed that to 110. I relaunched the job I was using to test,
and found that it was still setting up 220G&amp;hellip;WHY!?!!? well, it seems if you relaunch
in a step where the alignment has already begun, there is a cache for the resources.
So after more time trying to figure out this and asking myself, why I decided my
career path to be bioinformatics, I tried a fresh start and&amp;hellip;IT WORKED.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;How happy one can be to see a job requesting the expected resources.&lt;/p&gt;

&lt;p&gt;Incredible happy!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I went a step further, and looked how much memory actually it was using during the
alignment step, and I saw that actually only needs 64G, so I updated to 72G, to
gain more karma from the HPC world.&lt;/p&gt;

&lt;p&gt;There is one more thing, that will set jobs of 72G and 4 cores, if you want to
increase the number of cores, then use this option &lt;code&gt;--mempercore 6&lt;/code&gt; or whatever number
that you want: &lt;code&gt;mempercore = 72G/final_cores&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;workflow&#34;&gt;Workflow&lt;/h2&gt;

&lt;p&gt;Finally, I felt that I was doing my best to behave and avoid making angry other
people using the hpc. The final setup looks like this:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;meta-script to send all the samples to a master job: &lt;code&gt;runner.sh&lt;/code&gt;. It accepts
  a file where first column is &lt;code&gt;sample name&lt;/code&gt; and second column is &lt;code&gt;fastq_path&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;set -o pipefail  # trace ERR through pipes
set -o errtrace  # trace ERR through &#39;time command&#39; and other functions
set -o nounset   ## set -u : exit the script if you try to use an uninitialised variable
set -o errexit   ## set -e : exit the script if any statement returns a non-true return value
# set -v # you can uncomment all these lines to have a better debugging
# set -x
# export PS4=&#39;+(${BASH_SOURCE}:${LINENO}): ${FUNCNAME[0]:+${FUNCNAME[0]}(): }&#39;

while IFS=&amp;quot; &amp;quot; read -r sample paths

do

RUN=0
if [[ ! -e $sample/filtered_feature_bc_matrix.h5 ]]; then
RUN=1 # run if the final output is not there yet
fi

if [[  -e  $sample/_lock ]]; then
echo $sample is locked, please:
echo rm $sample/_lock
RUN=0 # but don&#39;t do anything if is locked
fi

if [[ $RUN == 1 ]]; then
echo sbatch -J $sample -o logs/$sample.o -e logs/$sample.e scripts/om-counts-cluster.slurm $sample $paths
sbatch -J $sample -o logs/$sample.o -e logs/$sample.e scripts/om-counts-cluster.slurm $sample $paths
fi

done &amp;lt; $1
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;SBATCH script with cellranger command: &lt;code&gt;counter.slurm&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/bash
#SBATCH -N 1
#SBATCH -c 1
#SBATCH --mem=6000
#SBATCH -t 0-48:00:00
# SBATCH -J &amp;quot;cellr-$1&amp;quot;
# SBATCH -e job-counts-$1.e
# SBATCH -o job-counts-$1.o
## SBATCH --mail-type=END,FAIL # this line is commented
## SBATCH --mail-user=user@mit.edu  # this line is commented

SCRIPTPATH=&amp;quot;PATHTOSCRIPTS&amp;quot;
cellranger count --id=$1 --transcriptome=GRCh38_pre_mRNA --fastqs=$2 --sample=$1 --jobmode=$SCRIPTPATH/slurm.template --maxjobs=3 --jobinterval=1000

if [ $? -eq 0 ]; then
echo OK
mv $1 results/.
else
echo $1 FAIL
fi
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;template script for &lt;code&gt;cellranger&lt;/code&gt; as shown above&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;other-tips&#34;&gt;Other tips&lt;/h3&gt;

&lt;p&gt;I used &lt;code&gt;mxjobs=3&lt;/code&gt; to avoid a lot of jobs. I am testing how things go, this could be spanned to bigger numbers.&lt;/p&gt;

&lt;p&gt;I used &lt;code&gt;jobinterval=1000&lt;/code&gt; to avoid sending jobs at the same time, just in case, but I don&amp;rsquo;t have any proof to support this.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;happiness in bioinformatics:&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
  </channel>
</rss>
